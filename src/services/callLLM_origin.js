const { OpenAI } = require('openai');

/**
 * LLMè°ƒç”¨æœåŠ¡
 * æ”¯æŒå¤šç§æ¨¡å‹å’Œæµå¼è¾“å‡º
 */
class LLMService {
  constructor() {
    this.openai = new OpenAI({ 
      apiKey: process.env.OPENAI_API_KEY 
    });
    this.defaultModel = process.env.OPENAI_MODEL || 'gpt-4';
    this.maxTokens = 4000;
    this.temperature = 0.7;
  }

  /**
   * è°ƒç”¨LLMç”Ÿæˆæ–‡æœ¬
   * @param {string} prompt - æç¤ºè¯
   * @param {Object} options - é…ç½®é€‰é¡¹
   * @returns {Promise<string>} ç”Ÿæˆçš„æ–‡æœ¬
   */
  async callLLM(prompt, options = {}) {
    try {
      const {
        model = this.defaultModel,
        temperature = this.temperature,
        maxTokens = this.maxTokens,
        systemPrompt = 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•ç”¨ä¾‹ç”ŸæˆåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æ¡£å†…å®¹ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚'
      } = options;

      const response = await this.openai.chat.completions.create({
        model: model,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: prompt }
        ],
        temperature: temperature,
        max_tokens: maxTokens,
        stream: false
      });

      const result = response.choices[0].message.content;
      console.log(`ğŸ¤– LLMè°ƒç”¨å®Œæˆ: ${model}, ç”Ÿæˆé•¿åº¦: ${result.length}`);
      
      return result;
    } catch (error) {
      console.error('LLMè°ƒç”¨é”™è¯¯:', error);
      throw new Error(`LLMè°ƒç”¨å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * æµå¼è°ƒç”¨LLM
   * @param {string} prompt - æç¤ºè¯
   * @param {Function} onChunk - å¤„ç†æ¯ä¸ªchunkçš„å›è°ƒ
   * @param {Object} options - é…ç½®é€‰é¡¹
   * @returns {Promise<void>}
   */
  async callLLMStream(prompt, onChunk, options = {}) {
    try {
      const {
        model = this.defaultModel,
        temperature = this.temperature,
        maxTokens = this.maxTokens,
        systemPrompt = 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•ç”¨ä¾‹ç”ŸæˆåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æ¡£å†…å®¹ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚'
      } = options;

      const stream = await this.openai.chat.completions.create({
        model: model,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: prompt }
        ],
        temperature: temperature,
        max_tokens: maxTokens,
        stream: true
      });

      let fullContent = '';
      
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        if (content) {
          fullContent += content;
          onChunk({
            type: 'chunk',
            content: content,
            fullContent: fullContent
          });
        }
      }

      onChunk({
        type: 'complete',
        content: '',
        fullContent: fullContent
      });

    } catch (error) {
      console.error('LLMæµå¼è°ƒç”¨é”™è¯¯:', error);
      onChunk({
        type: 'error',
        error: error.message
      });
      throw new Error(`LLMæµå¼è°ƒç”¨å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ç”Ÿæˆæµ‹è¯•è¦ç‚¹
   * @param {string} content - æ–‡æ¡£å†…å®¹
   * @returns {Promise<string>} æµ‹è¯•è¦ç‚¹
   */
  async generateTestKeyPoints(content) {
    const prompt = `è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–å…³é”®çš„æµ‹è¯•è¦ç‚¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. åŠŸèƒ½éœ€æ±‚
2. è¾¹ç•Œæ¡ä»¶
3. å¼‚å¸¸æƒ…å†µ
4. æ€§èƒ½è¦æ±‚
5. å®‰å…¨è¦æ±‚

æ–‡æ¡£å†…å®¹ï¼š
${content}

è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºæµ‹è¯•è¦ç‚¹ï¼š`;

    return await this.callLLM(prompt, {
      systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ–‡æ¡£ä¸­æå–æµ‹è¯•è¦ç‚¹ã€‚'
    });
  }

  /**
   * ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
   * @param {string} keyPoints - æµ‹è¯•è¦ç‚¹
   * @param {string} originalContent - åŸå§‹æ–‡æ¡£å†…å®¹
   * @returns {Promise<string>} æµ‹è¯•ç”¨ä¾‹
   */
  async generateTestCases(keyPoints, originalContent) {
    const prompt = `åŸºäºä»¥ä¸‹æµ‹è¯•è¦ç‚¹å’ŒåŸå§‹æ–‡æ¡£ï¼Œç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹ï¼š

æµ‹è¯•è¦ç‚¹ï¼š
${keyPoints}

åŸå§‹æ–‡æ¡£å†…å®¹ï¼š
${originalContent}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æµ‹è¯•ç”¨ä¾‹ï¼š
1. æµ‹è¯•ç”¨ä¾‹IDå’Œæ ‡é¢˜
2. å‰ç½®æ¡ä»¶
3. æµ‹è¯•æ­¥éª¤
4. é¢„æœŸç»“æœ
5. æµ‹è¯•æ•°æ®
6. ä¼˜å…ˆçº§

è¯·ä»¥è¡¨æ ¼æˆ–åˆ—è¡¨çš„å½¢å¼è¾“å‡ºï¼š`;

    return await this.callLLM(prompt, {
      systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œæ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ã€‚'
    });
  }

  /**
   * ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
   * @param {string} testCases - æµ‹è¯•ç”¨ä¾‹
   * @param {string} keyPoints - æµ‹è¯•è¦ç‚¹
   * @returns {Promise<string>} æµ‹è¯•æŠ¥å‘Š
   */
  async generateTestReport(testCases, keyPoints) {
    const prompt = `åŸºäºä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹å’Œæµ‹è¯•è¦ç‚¹ï¼Œç”Ÿæˆæµ‹è¯•æŠ¥å‘Šï¼š

æµ‹è¯•è¦ç‚¹ï¼š
${keyPoints}

æµ‹è¯•ç”¨ä¾‹ï¼š
${testCases}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æµ‹è¯•æŠ¥å‘Šï¼š
1. æµ‹è¯•æ¦‚è¿°
2. æµ‹è¯•èŒƒå›´
3. æµ‹è¯•ç­–ç•¥
4. é£é™©è¯„ä¼°
5. æµ‹è¯•å»ºè®®`;

    return await this.callLLM(prompt, {
      systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•ç»ç†ï¼Œæ“…é•¿ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šã€‚'
    });
  }

  /**
   * éªŒè¯APIå¯†é’¥
   * @returns {Promise<boolean>} æ˜¯å¦æœ‰æ•ˆ
   */
  async validateAPIKey() {
    try {
      await this.openai.models.list();
      return true;
    } catch (error) {
      console.error('APIå¯†é’¥éªŒè¯å¤±è´¥:', error);
      return false;
    }
  }
}

// åˆ›å»ºå•ä¾‹å®ä¾‹
const llmService = new LLMService();

module.exports = llmService; 